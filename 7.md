


### 问题背景


这是出现在某聘公司面试中的问题。因为在实际工作中，大部分工程师都会遇到一些棘手的特殊问题，他们可能由已有数据，选择的模型，以及硬件资源等因素决定。这也是验证你是否有过实际操作的开放问题之一。

---

![avatar](./img/bert1.png)

---

### 解题思路


#### 第一步: 确定需要涉猎的模型

* 微调BERT模型:
	* 这里我们选择带有语言模型头的BERT预训练模型，在自己构建的三层MLP网络下进行微调的过程。


---

#### 第二步: 阐述遇到的典型问题和解决办法

* 遇到的问题:
	* 模型在训练过程中，每batch的平均损失始终保持在0.6931的值附近（或恒等于0.6931），导致模型不再收敛，预测结果一般都指向固定标签。

---

* 出现原因:
	* 出现0.6931不收敛问题的内在原因是因为使用交叉熵损失时，In0.5=0.6931，也就是说模型在训练时落入了一个“自认为的”局部最优点，默认概率0.5是最稳妥的答案，而且很难通过梯度自适应的步长走出该区域。外在原因是由于模型前期接收的数据张量的复杂程度过高，如：每条数据都是768x300（词嵌入维度x文本对齐长度）的稠密矩阵，模型无法从批次数据中获得相应的规律。

---

* 什么情况下出现:
	* 该问题一般出现在使用大型预训练模型微调时，如Bert，XLNET的微调等，如果是完全的自构建模型，可以通过减小embedding和文本对齐长度，或适量增大batch_size的大小来解决对应的问题。而在使用预训练模型时，因为输出的维度是固定的且比较大，在长文本语料下，减小文本对齐长度将损失很多重要特征（不可取），增大batch_size的大小又很容易引起内存溢出。


---

* 解决办法:
	* 为了解决该问题，应该使用动态有序数据训练方法，即对训练数据进行处理，使的文本较短且规律明显的语料作为前几个批次的数据进入模型（注意：前期不要使用shuffle）长文本且规律不明显的样本放在语料后，这样如此几轮训练使得模型有了一定的“知识基础”，再在后边的轮次里shuffle数据以提升泛化能力。


---

    在我们微调带有三层MLP的BERT预训练模型时，模型训练时不收敛且损失一直在0.6931附近。最后我们使用了“动态有序数据训练”解决了该问题。

---



