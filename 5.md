


### 问题背景


这是出现在某网公司面试中的问题。可见该公司可能在某些业务中正在使用BERT模型，并希望通过该问题验证面试者对BERT模型的了解程度，以及在业务中是否使用一些方法来提升BERT模型的训练效率。

---

![avatar](https://github.com/AITutorials/manuals/blob/master/img/bert1.png)

---

### 解题思路


#### 第一步: 找出核心知识点并解析

* BERT模型:
>	* BERT也称双向Transformer编码器，它是由Transformer为基本单元构成的双向网络结构。BERT模型的创新点在于开启了迁移学习的篇章，它能够提供Masked LM，Next Sentence Prediction以及Question Answering任务的预训练模型，以增强在有限数据集下fine-tuning这些任务的效果。 

---

* 大型模型在训练过程中的优化措施:
>	* 1，数据并行（分布式）训练
>	* 2，模型并行（分布式）训练
>	* 3，数据与模型混合并行（分布式）训练

---

#### 第二步: 整合核心知识点形成答案


    在我们训练BERT模型的过程中，为了能够迭代调优方法，我们使用了数据并行的方式来加快训练。我们将模型copy到4台服务器上，然后使用将每个批次的数据4等分feed给模型处理，之后将每个模型的梯度在主节点上进行求和平均得到最终的更新梯度。通过这种方式，我们将模型的训练速度提升了约3.5倍，在10万文本数据集上训练的时间由原来的2h38min，减小到45min左右，为我们的整个模型从实验到上线节约了大量时间。

---

<!--

### 问题拓展

* 是否尝试使用Numba中JIT装饰器来进行代码加速
* 是否尝试使用cudf代替pandas来进行代码加速

---


-->
